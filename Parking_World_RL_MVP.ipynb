{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Parking_World_RL_MVP.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/11o9/A/blob/master/Parking_World_RL_MVP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "_KZ05dy2gBqw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Import C# Module"
      ]
    },
    {
      "metadata": {
        "id": "KbNSq7YCgBq0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import clr\n",
        "import os\n",
        "\n",
        "cwd = os.getcwd()\n",
        "\n",
        "sys.path.append(cwd+r\"C:\\..\\ParkingWorldMVP\\bin\\Debug\")\n",
        "\n",
        "import clr\n",
        "\n",
        "clr.AddReference(r\"clipper_library\")\n",
        "clr.AddReference(r\"Comar\")\n",
        "clr.AddReference(r\"ParkingWorldMVP\")\n",
        "clr.AddReference(r\"Rhino3dmIO\")\n",
        "\n",
        "from ParkingWorld import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sjpo5-VjgBq6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "env = PWEnvironment()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P806FwAGgBq8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qiMeV38-gBq_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def one_hot(x):\n",
        "    return np.identity(16)[x:x+1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V98e36jegBrC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "input_size = env.NUM_STATE_FEATURE\n",
        "output_size = env.NUM_ACTION\n",
        "learning_rate = 0.1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uiEQrc6LgBrE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = tf.placeholder(shape = [1,input_size], dtype = tf.float32)\n",
        "W = tf.Variable(tf.random_uniform([input_size, output_size],0,0.01))\n",
        "Qpred = tf.matmul(X,W)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_5gzaKJAgBrG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Y = tf.placeholder(shape = [1, output_size], dtype = tf.float32)\n",
        "\n",
        "loss = tf.reduce_sum(tf.square(Y-Qpred))\n",
        "\n",
        "train = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
        "\n",
        "dis = .99\n",
        "num_episodes = 1000\n",
        "rList = []\n",
        "\n",
        "sess = tf.Session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q2ifA3dIgBrI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## START LEARNING"
      ]
    },
    {
      "metadata": {
        "id": "n1slR68EgBrK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "#print(sess.run(W))\n",
        "\n",
        "#Start Episode\n",
        "for i in range(num_episodes):\n",
        "\n",
        "    #print(\"episode \" ,i)\n",
        "\n",
        "    #Starting State that is always zero\n",
        "    s = env.Reset()\n",
        "    #print(\"start state : \",s)\n",
        "\n",
        "    #Epsillon\n",
        "    e = 1. / ((i/50)+10)\n",
        "    rAll = 0\n",
        "    done = False\n",
        "    local_loss = []\n",
        "\n",
        "    while not done:\n",
        "\n",
        "        Qs =sess.run(Qpred,feed_dict={X : one_hot(s)})\n",
        "        #print(\"Start Qs : \" ,Qs)\n",
        "        if np.random.rand(1) <e:\n",
        "            a = env.action_space.sample()\n",
        "        else:\n",
        "            a = np.argmax(Qs)\n",
        "\n",
        "        #print(\"selected action : \",a)\n",
        "        s1,reward,done,_ = env.step(a)\n",
        "        #print(\"next sate: \",s1 ,\" | reward : \", reward, \" | done : \", done)\n",
        "\n",
        "\n",
        "        if done:\n",
        "            Qs[0,a] =reward\n",
        "            #print(Qs)\n",
        "        else:    \n",
        "            Qs1 = sess.run(Qpred, feed_dict = { X : one_hot(s1)})\n",
        "            Qs[0,a] = reward + dis * np.max(Qs1)\n",
        "\n",
        "        #print(\"End Qs : \" ,Qs)\n",
        "\n",
        "        sess.run(train, feed_dict = {X:one_hot(s),Y:Qs})\n",
        "\n",
        "        rAll += reward\n",
        "        s =s1\n",
        "\n",
        "        #print(\"\")\n",
        "\n",
        "    rList.append(rAll)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}